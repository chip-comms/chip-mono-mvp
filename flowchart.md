# Virtual Meeting Assistant MVP (Local Prototype)

## Overview
A minimal **local-only** web application to test the core value proposition: upload a meeting recording ‚Üí automatically generate transcript, summary, action items, and insights. **No auth, no database, no deployment** - just the essential flow running on your machine to validate the AI intelligence quality.

---

## Core Components

### 1. Frontend (Next.js App)
**Tech Stack**: Next.js + TypeScript + Tailwind CSS

**Single Page App**:
- Upload section at top
- Recordings list below (reads from local JSON file)
- Click to expand intelligence inline
  
**Upload Section**:
- Drag & drop or file picker for video/audio files
- Support formats: MP4, WebM, MOV, MP3, WAV
- Uploads to Next.js API route
- Shows processing status
- File size validation (max 200MB)
  
**Recordings List**:
- Reads from `data/recordings.json`
- Status badges: "Processing", "Completed", "Failed"
- Click to expand and view intelligence
  
**Intelligence View**:
- Video/audio player (serves from `public/uploads/`)
- Tabs: 
  - Summary: AI-generated overview
  - Transcript: Full text with timestamps
  - Action Items: Extracted tasks with priority
  - Communication Metrics: Talk time %, response delays, company values alignment
  - Topics & Sentiment: Key topics discussed and overall sentiment
- Download transcript button

---

### 2. Backend (Next.js API Routes)

**API Routes**:

#### `POST /api/upload`
- Receives file from frontend
- Saves to `public/uploads/{filename}`
- Creates entry in `data/recordings.json`
- Triggers processing (async)
- Returns recording ID

#### `POST /api/process`
- Called automatically after upload
- Extracts audio if video (using FFmpeg)
- Calls OpenAI Whisper for transcription
- Calls OpenAI GPT-4o-mini for analysis
- Saves results to `data/intelligence/{recording_id}.json`
- Updates status in `data/recordings.json`

#### `GET /api/recordings`
- Returns list of all recordings from JSON file

#### `GET /api/intelligence/:id`
- Returns intelligence data for specific recording

---

### 3. Local Storage (No Database)

**File Structure**:
```
project/
‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îî‚îÄ‚îÄ uploads/           # Uploaded video/audio files
‚îÇ       ‚îú‚îÄ‚îÄ abc123.mp4
‚îÇ       ‚îî‚îÄ‚îÄ def456.wav
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ recordings.json    # List of recordings with metadata
‚îÇ   ‚îî‚îÄ‚îÄ intelligence/      # Individual intelligence files
‚îÇ       ‚îú‚îÄ‚îÄ abc123.json
‚îÇ       ‚îî‚îÄ‚îÄ def456.json
```

**`data/recordings.json`**:
```json
[
  {
    "id": "abc123",
    "title": "Team Standup - Oct 7.mp4",
    "filename": "abc123.mp4",
    "fileType": "video/mp4",
    "fileSizeBytes": 45000000,
    "status": "completed",
    "createdAt": "2025-10-07T10:00:00Z"
  }
]
```

**`data/intelligence/abc123.json`**:
```json
{
  "recordingId": "abc123",
  "transcript": "Speaker 1: Good morning everyone...",
  "summary": "The team discussed...",
  "actionItems": [
    {"text": "Deploy to staging", "priority": "high"}
  ],
  "keyTopics": [
    {"topic": "Sprint Planning", "relevance": 0.9}
  ],
  "sentiment": {
    "overall": "positive",
    "score": 0.7
  },
  "speakerStats": {
    "speakers": [
      {"name": "Speaker 1", "duration": 120, "wordCount": 450}
    ]
  },
  "communicationMetrics": {
    "talkTimePercentage": 35,
    "responseDelays": [
      {"afterSpeaker": "Speaker 2", "delaySeconds": 0.5}
    ],
    "eyeContact": null,
    "companyValuesAlignment": {
      "values": [
        {"value": "Collaboration", "score": 0.8, "examples": ["..."]}
      ]
    }
  }
}
```

---

## Communication Intelligence Metrics

### What We Want to Analyze

#### 1. **Talk Time Percentage** üìä
**Question**: What percent of the meeting time are you talking vs others?

**Data Source**: Audio transcript with speaker diarization + timestamps

**Implementation** (Audio-only):
- Parse transcript timestamps to calculate duration per speaker
- Calculate: `(your_talk_time / total_meeting_time) * 100`
- Compare to other speakers' percentages
- Ideal range: Depends on role (manager vs IC, meeting type)

**Output**:
```json
{
  "talkTimePercentage": 35,
  "breakdown": {
    "you": 35,
    "others": 65
  },
  "speakerBreakdown": [
    {"speaker": "You", "percentage": 35, "duration": 420},
    {"speaker": "Speaker 2", "percentage": 45, "duration": 540},
    {"speaker": "Speaker 3", "percentage": 20, "duration": 240}
  ]
}
```

---

#### 2. **Response Delay** ‚è±Ô∏è
**Question**: How long do you wait to respond after someone else finishes talking?

**Data Source**: Audio transcript with precise timestamps

**Implementation** (Audio-only):
- Track timestamp when speaker finishes
- Track timestamp when you start speaking
- Calculate gap: `your_start_time - previous_speaker_end_time`
- Analyze patterns: Are you interrupting? Waiting too long?

**Output**:
```json
{
  "averageResponseDelay": 1.2,
  "responseDelays": [
    {"afterSpeaker": "Speaker 2", "delaySeconds": 0.5, "context": "Quick agreement"},
    {"afterSpeaker": "Speaker 3", "delaySeconds": 2.1, "context": "Thoughtful response"},
    {"afterSpeaker": "Speaker 2", "delaySeconds": -0.3, "context": "Interruption"}
  ],
  "interruptions": 1,
  "insights": "You tend to interrupt Speaker 2. Consider waiting 1-2 seconds."
}
```

---

#### 3. **Eye Contact Quality** üëÅÔ∏è
**Question**: How good is your eye contact during the meeting?

**Data Source**: Video analysis (camera feed)

**Implementation** (Requires video - NOT in audio-only prototype):
- Use computer vision to detect face and gaze direction
- Track when you're looking at camera vs away
- Calculate percentage of time with "good" eye contact
- Detect patterns: Looking away during speaking vs listening

**Status**: ‚ùå **NOT FEASIBLE with audio-only prototype**

**Future Implementation**:
- Would require video processing
- Face detection + gaze estimation models
- Compare to meeting context (presenting vs listening)

**Output** (future):
```json
{
  "eyeContactPercentage": 65,
  "patterns": {
    "whileSpeaking": 70,
    "whileListening": 60
  },
  "insights": "Good eye contact while speaking. Try to maintain more while listening."
}
```

---

#### 4. **Company Values Alignment** üéØ
**Question**: Are you demonstrating/living according to company values?

**Data Source**: Audio transcript + AI analysis

**Implementation** (Audio-only with AI):
- Define company values (e.g., "Collaboration", "Innovation", "Customer Focus")
  - For prototype: hardcode 3-5 common values
  - For production: user can configure their company's values
- Use GPT-4 to analyze transcript for examples of each value
- Score alignment for each value (0-1)
- Extract specific quotes/examples

**Output**:
```json
{
  "overallAlignment": 0.75,
  "values": [
    {
      "value": "Collaboration",
      "score": 0.85,
      "examples": [
        "You asked for team input: 'What does everyone think about this approach?'",
        "You built on Speaker 2's idea: 'I like that suggestion, we could extend it by...'"
      ]
    },
    {
      "value": "Innovation",
      "score": 0.70,
      "examples": [
        "You proposed a new solution: 'What if we tried using...'"
      ]
    },
    {
      "value": "Customer Focus",
      "score": 0.60,
      "examples": [
        "You mentioned customer needs once during discussion"
      ]
    }
  ],
  "insights": "Strong collaboration shown. Consider mentioning customer needs more often."
}
```

---

### Summary: Feasibility with Audio-Only

| Metric | Audio-Only? | Complexity | Priority for MVP |
|--------|-------------|------------|------------------|
| **Talk Time %** | ‚úÖ Yes | Low | üî• High |
| **Response Delay** | ‚úÖ Yes | Medium | üî• High |
| **Eye Contact** | ‚ùå No (needs video) | High | ‚≠ê Future V2 |
| **Company Values** | ‚úÖ Yes | Medium | üî• High |

### Implementation Plan

**Phase 1 (Audio-only prototype)**:
1. ‚úÖ Get basic transcription with Whisper
2. ‚úÖ Extract speaker diarization (who spoke when)
3. ‚úÖ Calculate talk time percentage
4. ‚úÖ Calculate response delays (gaps between speakers)
5. ‚úÖ Use GPT-4 to analyze company values alignment

**Phase 2 (After prototype works)**:
- Add video analysis for eye contact
- Improve speaker identification (names, not just "Speaker 1")
- Add trends over time (track metrics across multiple meetings)

---

## User Flow (Local Prototype)

### 1. Start App
- Run `npm run dev` in terminal
- Open `http://localhost:3000`
- See upload section + list of previous recordings (from JSON file)

### 2. Upload Recording
- Drag & drop video/audio file (or click to select)
- Frontend:
  1. Validates file type and size (< 200MB)
  2. POSTs to `/api/upload`
  3. Shows upload progress bar
- Backend (`/api/upload`):
  1. Saves file to `public/uploads/`
  2. Creates entry in `data/recordings.json` with status "processing"
  3. Automatically triggers `/api/process` in background
  4. Returns recording ID to frontend

### 3. Processing (Automatic)
- `/api/process` runs in background:
  1. Reads file from `public/uploads/`
  2. Extracts audio if video (FFmpeg)
  3. Calls OpenAI Whisper API for transcription (~30 sec)
  4. Calls OpenAI GPT-4o-mini for analysis (~15 sec)
  5. Saves results to `data/intelligence/{id}.json`
  6. Updates status in `data/recordings.json` to "completed"
- Frontend polls `/api/recordings` every 3 seconds
- Shows spinner with "Processing..."
- **Processing time**: ~1-3 min for 10-min recording

### 4. View Intelligence
- When status = "completed", recording card shows results
- User clicks to expand inline
- Shows:
  - Video/audio player (serves from `/uploads/`)
  - Tabs:
    - **Summary**: AI-generated overview of discussion
    - **Transcript**: Full text with timestamps
    - **Action Items**: Extracted tasks with priorities
    - **Communication Metrics**: 
      - Talk time percentage (you vs others)
      - Response delays (how quickly you respond)
      - Company values alignment with examples
    - **Topics & Sentiment**: Key topics and overall sentiment
  - Download transcript button
- User can:
  - Play video while reading transcript
  - Copy action items
  - Review communication patterns
  - Close and reopen to view again

### 5. Multiple Recordings
- Upload more files - they all save to local filesystem
- All recordings persist in `data/recordings.json`
- Refresh page and they're still there
- **Note**: To clear, just delete files in `public/uploads/` and `data/`

---

## Technical Architecture (Local-Only)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Browser (localhost:3000)               ‚îÇ
‚îÇ                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Next.js Frontend                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - File Upload Component                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Recordings List                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Intelligence Viewer                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Video Player                          ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ           ‚îÇ                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚îÇ HTTP requests
            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Next.js Dev Server (localhost:3000)          ‚îÇ
‚îÇ                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  API Routes                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  POST /api/upload                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚Üí Save file to public/uploads/        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚Üí Write to data/recordings.json       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚Üí Trigger /api/process                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  POST /api/process                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚Üí Read file from public/uploads/      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚Üí Call Whisper API ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚Üí Call GPT-4o-mini API ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚Üí Save to data/intelligence/   ‚îÇ       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚Üí Update data/recordings.json  ‚îÇ       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                   ‚îÇ       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  GET /api/recordings              ‚îÇ       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚Üí Read data/recordings.json    ‚îÇ       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                   ‚îÇ       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  GET /api/intelligence/:id        ‚îÇ       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚Üí Read data/intelligence/{id}  ‚îÇ       ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Local Filesystem Storage                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  public/uploads/                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ abc123.mp4                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚îî‚îÄ def456.wav                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  data/recordings.json                    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  data/intelligence/                      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ abc123.json                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚îî‚îÄ def456.json                        ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚îÇ External API calls
            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         OpenAI API (api.openai.com)            ‚îÇ
‚îÇ                                                ‚îÇ
‚îÇ  ‚Ä¢ Whisper API ‚Üí Transcription                ‚îÇ
‚îÇ  ‚Ä¢ GPT-4o-mini API ‚Üí Intelligence             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Simple Data Flow:

```
1. Upload
   User drops file ‚Üí POST /api/upload
      ‚Üí Saves to public/uploads/abc123.mp4
      ‚Üí Writes to data/recordings.json (status: "processing")
      ‚Üí Triggers POST /api/process (background)
      ‚Üí Returns {id: "abc123"}

2. Process (background)
   /api/process runs
      ‚Üí Reads public/uploads/abc123.mp4
      ‚Üí Extracts audio (FFmpeg if video)
      ‚Üí Calls Whisper API (~30 sec)
      ‚Üí Calls GPT-4o-mini API (~15 sec)  
      ‚Üí Writes data/intelligence/abc123.json
      ‚Üí Updates data/recordings.json (status: "completed")

3. View
   Frontend polls GET /api/recordings every 3 sec
      ‚Üí Sees status = "completed"
      ‚Üí Fetches GET /api/intelligence/abc123
      ‚Üí Displays transcript, summary, action items
      ‚Üí User plays video from /uploads/abc123.mp4
```

**Ultra-Simple Stack**:
- ‚úÖ **Frontend**: Next.js + TypeScript + Tailwind
- ‚úÖ **Backend**: Next.js API routes (Node.js)
- ‚úÖ **Storage**: Local filesystem (no S3, no Supabase)
- ‚úÖ **Data**: JSON files (no database)
- ‚úÖ **Processing**: FFmpeg + OpenAI APIs
- ‚ùå **No**: Auth, deployment, cloud services

---

## Prototype Scope Decisions

### Included in Local Prototype:
‚úÖ File upload (drag & drop)
‚úÖ Save to local filesystem (`public/uploads/`)
‚úÖ AI transcription (OpenAI Whisper with speaker diarization)
‚úÖ AI-generated summaries (GPT-4o-mini)
‚úÖ Action items extraction
‚úÖ Key topics identification
‚úÖ Sentiment analysis
‚úÖ **Communication metrics** (audio-only):
  - Talk time percentage (you vs others)
  - Response delays (timing between speakers)
  - Company values alignment analysis
‚úÖ Single-page interface
‚úÖ Video/audio playback
‚úÖ Transcript viewer with timestamps
‚úÖ Status polling
‚úÖ JSON file storage (no database)
‚úÖ Next.js API routes (no edge functions)

### Excluded from Prototype:
‚ùå User authentication
‚ùå Database (Supabase/Postgres)
‚ùå Cloud storage (S3/Supabase Storage)
‚ùå Deployment (Vercel/hosting)
‚ùå Row Level Security
‚ùå Search and filter
‚ùå Delete functionality (just delete files manually)
‚ùå Real-time subscriptions
‚ùå Error recovery
‚ùå Email notifications
‚ùå Responsive design
‚ùå Production optimizations
‚ùå **Eye contact analysis** (requires video processing)
‚ùå **Video-based metrics** (facial expressions, body language)

### Add After Local Testing Works:
1. Deploy to Vercel + Supabase
2. Add authentication
3. Replace JSON files with database
4. Add proper error handling
5. Polish UI/UX

### Future V2+ Features:
- Chrome extension for live recording
- Calendar integration
- Auto-join meetings
- Advanced speaker identification (names, not just "Speaker 1")
- **Video analysis**:
  - Eye contact tracking
  - Facial expressions analysis
  - Body language patterns
  - Screen sharing content analysis
- Multi-language support
- Team collaboration features
- Trends over time (track metrics across meetings)

---

## Key Technical Challenges & Solutions

### Challenge 1: Large File Uploads
**Problem**: Video files can be 100MB-1GB+, slow to upload
**Solution**: 
- Use Supabase Storage resumable uploads (handles interruptions)
- Show real-time upload progress (percentage + speed)
- Client-side validation (file size limit 500MB for MVP)
- Compress video on client-side before upload (optional, using FFMPEG.wasm)
- Support common formats: MP4, WebM, MOV, MP3, WAV

### Challenge 2: Processing Time & User Experience
**Problem**: Transcription + AI analysis takes 2-5 minutes, users might leave
**Solution**:
- Async processing with edge functions
- Real-time status updates using Supabase Realtime subscriptions
- Email notification when processing completes (optional)
- Show estimated processing time based on file duration
- Allow users to navigate away and check back later

### Challenge 3: AI Processing Cost
**Problem**: Transcription + GPT-4 calls are expensive (~$0.50-$2 per hour of audio)
**Solution**:
- Start with OpenAI Whisper (cheaper: $0.006/min = $0.36/hour)
- Use GPT-4o-mini for analysis (cheaper than GPT-4)
- Implement usage limits per user (e.g., 10 recordings/month free tier)
- Cache transcripts and intelligence results
- Consider Deepgram as alternative (flat-rate pricing)

### Challenge 4: Audio Extraction from Video
**Problem**: AI transcription services need audio-only files
**Solution**:
- Use FFmpeg in edge function to extract audio from video
- Convert to standard format (MP3 or WAV)
- Cache extracted audio to avoid re-processing
- For audio-only uploads, skip this step

### Challenge 5: Privacy & Security
**Problem**: Meeting recordings contain sensitive business information
**Solution**:
- Strict Row Level Security (RLS) policies on all tables
- Storage bucket access restricted to authenticated users only
- Presigned URLs with 1-hour expiry for video playback
- Optional: End-to-end encryption (future feature)
- Clear privacy policy about AI processing
- Data retention controls (auto-delete after X days)
- GDPR compliance: User can delete all data on demand

### Challenge 6: Transcript Accuracy
**Problem**: AI transcription isn't perfect, especially with accents/jargon
**Solution**:
- Use highest-quality Whisper model (or Deepgram premium)
- Display confidence scores (if available)
- Allow manual editing of transcripts (future feature)
- Pre-process audio: noise reduction, normalization
- Set user expectations ("90%+ accuracy")

---

## MVP Metrics for Success

### 1. Technical Metrics:
- ‚úÖ 95%+ successful uploads (no corruption/failures)
- ‚úÖ < 5 min processing time for 30-min recording
- ‚úÖ 90%+ transcription accuracy (Whisper standard)
- ‚úÖ < 3 sec page load time
- ‚úÖ Real-time status updates work reliably

### 2. User Engagement:
- üéØ 10+ beta users test the app
- üéØ Average 2+ uploads per user per week
- üéØ 80%+ users view the intelligence report after upload
- üéØ Average session time: 5+ minutes
- üéØ Return rate: 50%+ users come back within 7 days

### 3. Feature Usage:
- üéØ 90%+ users successfully upload their first file
- üéØ 70%+ users try the transcript viewer
- üéØ 50%+ users download transcript or use action items
- üéØ 30%+ users upload both video and audio files

### 4. Value Validation:
- üéØ User feedback: "Would you use this product?" ‚Üí 70%+ say yes
- üéØ NPS Score: 30+ (good for MVP)
- üéØ Willingness to pay: 40%+ would pay $10-20/month
- üéØ Time saved: Users report saving 10+ min per recording on manual note-taking

---

## Development Phases (Ultra-Fast Local Prototype)

### Day 1: Setup (2-3 hours)
**Goal**: Basic Next.js app running

- Create Next.js project: `npx create-next-app@latest`
- Install dependencies: `openai`, `fluent-ffmpeg`
- Create folder structure: `public/uploads/`, `data/intelligence/`
- Create empty `data/recordings.json` file
- Test `npm run dev` works

**Deliverable**: App loads at localhost:3000

---

### Day 2: Upload Flow (4-5 hours)
**Goal**: Can upload and save files

- Build upload component (drag & drop using `react-dropzone`)
- Create `/api/upload` route
  - Parse multipart form data
  - Save file to `public/uploads/`
  - Add entry to `data/recordings.json`
- Display list of recordings (read from JSON)
- Test uploading a video

**Deliverable**: Upload video, see it in list

---

### Day 3: AI Processing (6-8 hours)
**Goal**: Backend calls OpenAI APIs

- Install FFmpeg locally (`brew install ffmpeg` on Mac)
- Create `/api/process` route
  - Extract audio from video using fluent-ffmpeg
  - Call OpenAI Whisper API for transcription
  - Call OpenAI GPT-4o-mini for analysis
  - Save to `data/intelligence/{id}.json`
  - Update status in `data/recordings.json`
- Test with small audio file first
- Add `.env.local` with `OPENAI_API_KEY`

**Deliverable**: Process button generates intelligence

---

### Day 4: Display Intelligence (4-5 hours)
**Goal**: Show AI results in UI

- Create expandable recording card component
- Create tabbed view: Summary, Transcript, Action Items
- Add HTML5 video player
- Create `/api/intelligence/:id` route
- Implement polling (check status every 3 sec)
- Style with Tailwind

**Deliverable**: Complete flow works end-to-end

---

### Day 5: Polish & Test (3-4 hours)
**Goal**: Fix bugs, test with real recordings

- Test with 2-3 real meeting recordings
- Handle errors (show error messages)
- Add loading spinners and status badges
- Improve GPT prompt for better insights
- Add download transcript button
- Write simple README

**Deliverable**: Working local prototype

**Total Time: 5 days** (or 20-25 hours)

---

## Tech Stack Summary

| Component | Technology | Purpose |
|-----------|-----------|---------|
| Frontend | Next.js 14 (App Router) | React framework |
| UI Library | React 18 | Component library |
| Styling | Tailwind CSS | Utility-first CSS |
| Language | TypeScript | Type safety |
| Backend | Next.js API Routes | Node.js serverless functions |
| Storage | Local Filesystem | Store files in `public/uploads/` |
| Database | JSON Files | Store data in `data/*.json` |
| AI - Transcription | OpenAI Whisper API | Speech-to-text |
| AI - Analysis | OpenAI GPT-4o-mini | Text analysis & summarization |
| Video Processing | FFmpeg (local install) | Audio extraction |
| Dev Server | Next.js Dev | `npm run dev` |
| Environment | Node.js 18+ | Runtime |

---

## Quick Start (Local Development Only)

### 1. Prerequisites
```bash
# Install these first:
- Node.js 18+ (https://nodejs.org)
- FFmpeg (brew install ffmpeg on Mac, or download from ffmpeg.org)
- OpenAI API key (https://platform.openai.com/api-keys)
```

### 2. Create Next.js Project (5 min)
```bash
# Create new Next.js app
npx create-next-app@latest meeting-assistant --typescript --tailwind --app

cd meeting-assistant

# Install dependencies
npm install openai fluent-ffmpeg formidable

# Install dev dependencies
npm install -D @types/fluent-ffmpeg @types/formidable
```

### 3. Setup Environment (2 min)
```bash
# Create .env.local
echo "OPENAI_API_KEY=sk-your-key-here" > .env.local

# Create folder structure
mkdir -p public/uploads
mkdir -p data/intelligence

# Create empty recordings file
echo "[]" > data/recordings.json
```

### 4. Add to .gitignore
```bash
# Add these lines to .gitignore
public/uploads/
data/
```

### 5. Run Development Server
```bash
npm run dev

# Open http://localhost:3000
```

### 6. Test the Flow
```bash
1. Open http://localhost:3000
2. Drag & drop a short meeting video (< 100MB)
3. Watch upload progress
4. Wait ~1-2 min for processing
5. Click to expand and view transcript, summary, action items
```

### Project Structure
```
meeting-assistant/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ page.tsx              # Main page (upload + list)
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ upload/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ route.ts      # POST /api/upload
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ process/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ route.ts      # POST /api/process
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recordings/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ route.ts      # GET /api/recordings
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ intelligence/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ [id]/
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ route.ts  # GET /api/intelligence/:id
‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îî‚îÄ‚îÄ uploads/              # Video/audio files
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ recordings.json       # Recording metadata
‚îÇ   ‚îî‚îÄ‚îÄ intelligence/         # Intelligence JSON files
‚îú‚îÄ‚îÄ .env.local                # OPENAI_API_KEY
‚îî‚îÄ‚îÄ package.json
```

---

## Cost Estimation (Local Prototype)

### Development Cost:
- **Infrastructure**: $0 (running locally)
- **OpenAI API** (for testing):
  - Whisper transcription: $0.006/min
  - GPT-4o-mini: ~$0.15 per 1M input tokens, ~$0.60 per 1M output tokens
  
### Example Test Costs:
- **10-min audio recording**:
  - Whisper: $0.06 (10 min)
  - GPT-4o-mini: ~$0.02 (5K tokens)
  - **Total: ~$0.08 per test**
  
- **Testing budget** (10 test recordings):
  - ~$0.80 total for all testing
  - **Budget: < $2**

### If You Deploy Later:
- Vercel: Free tier
- Supabase: Free tier
- OpenAI: Pay-as-you-go
- **Total infrastructure: $0** (until you hit limits)

---

## Next Steps After MVP

### Immediate Improvements (V1.1):
1. **Better error handling**: Show specific errors, retry options
2. **Email notifications**: "Your recording is ready!"
3. **Transcript editing**: Allow users to fix transcription errors
4. **Export options**: PDF summary, DOCX transcript
5. **Usage analytics**: Show user their stats (hours processed, words transcribed)

### Feature Additions (V2.0):
1. **Chrome extension**: Record directly from browser
2. **Calendar integration**: Auto-fetch meeting recordings
3. **Team features**: Share recordings within organization
4. **AI chat**: Ask questions about your meetings
5. **Meeting bot**: Send bot to join meetings on your behalf
6. **Integrations**: Export action items to Notion, Asana, Linear

### Scale Improvements (V3.0):
1. **Batch processing**: Process multiple files efficiently
2. **Custom AI models**: Fine-tuned for your industry/terminology
3. **Real-time transcription**: Show live captions during recording
4. **Multi-language**: Support 10+ languages
5. **Video analysis**: Detect slides, extract text, identify speakers
6. **Mobile apps**: iOS/Android for on-the-go uploads

---

## Conclusion

This **local-first prototype** is the **absolute simplest** way to test if AI-generated meeting intelligence is actually useful. No deployment, no database, no auth‚Äîjust run it on your machine and see if the AI delivers value.

**Why start local?**
- ‚úÖ **Ultra-fast**: Build in 5 days (20-25 hours)
- ‚úÖ **Zero infrastructure**: No cloud setup, no deployment
- ‚úÖ **Minimal cost**: < $2 for all testing
- ‚úÖ **Easy debugging**: Everything runs on localhost
- ‚úÖ **Quick iteration**: Change code, refresh page
- ‚úÖ **No commitments**: If it doesn't work, you've only spent a weekend

**What you'll learn:**
1. Is Whisper transcription (with speaker diarization) accurate enough?
2. Are GPT-4o-mini's action items useful?
3. Are the communication metrics (talk time, response delays) insightful?
4. Does company values alignment analysis provide value?
5. Is the processing time acceptable (~1-2 min)?
6. Do you want to keep using it yourself?
7. Is this worth building out fully?

**Decision Tree After Testing:**

```
‚úÖ It works great & I use it daily
   ‚Üí Add auth + database
   ‚Üí Deploy to Vercel + Supabase
   ‚Üí Add calendar integration
   ‚Üí Build Chrome extension

ü§î It's okay, but needs improvement
   ‚Üí Iterate on AI prompts
   ‚Üí Try different models (Claude?)
   ‚Üí Test with more meeting types
   ‚Üí Get feedback from 3-5 people

‚ùå It doesn't deliver value
   ‚Üí Stop now, save months of work
   ‚Üí Pivot to different approach
   ‚Üí No sunk cost fallacy
```

**This prototype answers ONE question:**
> "If I upload a meeting recording, does the AI generate useful insights that save me time?"

If yes ‚Üí build more. If no ‚Üí stop or pivot. That's it.

---

## Getting Started

**Time to working prototype: 20-25 hours**

```bash
# Weekend 1: Core functionality (15 hours)
Day 1 (3h):  Setup Next.js + install dependencies
Day 2 (5h):  Upload flow + save to filesystem
Day 3 (7h):  AI processing (Whisper + GPT-4o-mini)

# Weekend 2: Polish & test (8 hours)
Day 4 (5h):  Display intelligence in UI
Day 5 (3h):  Test with real recordings + fix bugs
```

**Budget**: 
- Infrastructure: $0
- OpenAI API: < $2
- **Total: ~$2**

**Success = You answer this:**
"Is this useful enough that I would pay $15/month for it?"

If yes, keep building. If no, you learned early. üöÄ
